import asyncio
import json
import os
import traceback

import requests
from dotenv import load_dotenv
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from portkey_ai import PORTKEY_GATEWAY_URL, createHeaders

from app.core.constants import (
    DATASETS_USED,
    INTERMEDIATE_MESSAGES,
    SQL_QUERIES_GENERATED,
)

from .multi_dataset_cases import COMPLEX_QUERY_CASES
from .single_dataset_cases import SINGLE_DATASET_TEST_CASES

load_dotenv()

url = "http://localhost:8001/api/v1/chat/completions"


def setup_model():
    api_key = os.getenv("PORTKEY_API_KEY")
    virtual_key = os.getenv("GEMINI_VIRTUAL_KEY")

    model = ChatOpenAI(
        api_key="X",  # type: ignore
        base_url=PORTKEY_GATEWAY_URL,
        default_headers=createHeaders(
            api_key=api_key,
            virtual_key=virtual_key,
        ),
        model="gemini-2.5-flash-preview-04-17",
    )
    return model


def create_chain():
    template = """
        You are a judgemental data analyst assistant.

        You will be given:
        1. The answer generated by our system in response to a user query
        2. Details about the expected result

        generated_answer: {generated_answer}
        expected_result: {expected_result}

        The expected_result may contain some or all of these details:
        - dataset_identified: which dataset should be used (only present for multi-dataset cases)
        - sql_query_count: number of SQL queries expected
        - visualization_needed: whether visualization is required
        - visualization_type: what type of visualization is appropriate

        Your task is to evaluate if the generated answer meets the expected criteria:

        RETURN 'true' if:
        - The answer correctly identified the expected dataset (if dataset_identified is specified)
        - The number of SQL queries used is appropriate (matching sql_query_count if specified)
        - Visualization recommendations (if applicable) match the expected result
        - The numerical values are within acceptable ranges
        - The SQL queries were properly extracted and reported in the response

        RETURN 'false' if:
        - The answer identified the wrong dataset (when dataset_identified is specified)
        - The SQL query count is significantly off
        - Visualization was needed but not recommended (or vice versa)
        - The answer is factually incorrect
        - No SQL queries were detected when they should have been

        RETURN 'partial' if:
        - The answer is factually correct but there are minor issues with:
          - Dataset identification (when applicable)
          - SQL query count
          - Visualization recommendations
        - Or if the answer is partially correct but missing some components

        RETURN the response as JSON object with the following keys:
        {{
            "correct": "true" | "false" | "partial",
            "reasoning": "reasoning for why the answer is incorrect or partial,
                          no reasoning needed for correct answers",
        }}
    """

    prompt = ChatPromptTemplate.from_template(template)
    parser = JsonOutputParser()
    llm = setup_model()
    return prompt | llm | parser


def process_tool_calls(tool_calls):
    tool_messages = []
    generated_sql_queries = []
    selected_datasets = []

    if not tool_calls:
        return tool_messages, generated_sql_queries, selected_datasets

    for tool_call in tool_calls:
        if not tool_call or "function" not in tool_call:
            continue

        function_data = tool_call.get("function", {})
        name = function_data.get("name")

        try:
            args = json.loads(function_data.get("arguments", "{}"))

            if name == SQL_QUERIES_GENERATED:
                if "query" in args:
                    query = args.get("query")
                    generated_sql_queries.append(query)
                elif "queries" in args:
                    queries = args.get("queries")
                    if isinstance(queries, list):
                        generated_sql_queries.extend(queries)
                    else:
                        query = str(queries)
                        generated_sql_queries.append(query)

            elif name == DATASETS_USED:
                datasets = args.get("datasets", [])
                if datasets:
                    if isinstance(datasets, list):
                        selected_datasets.extend(datasets)
                    else:
                        selected_datasets.append(str(datasets))

            elif name == INTERMEDIATE_MESSAGES:
                content = args.get("content", "")
                category = args.get("category", "")
                if args.get("role") == "intermediate":
                    tool_messages.append(content)

                    if category == DATASETS_USED or (
                        content and "dataset" in content.lower()
                    ):
                        if "datasets" in args:
                            datasets = args.get("datasets")
                            if isinstance(datasets, list):
                                selected_datasets.extend(datasets)
                            else:
                                selected_datasets.append(str(datasets))
                        elif ":" in content:
                            if (
                                "using dataset" in content.lower()
                                or "selected dataset" in content.lower()
                            ):
                                datasets_part = content.split(":", 1)[
                                    1
                                ].strip()
                                selected_datasets.append(datasets_part)

                    if category == SQL_QUERIES_GENERATED or (
                        content
                        and (
                            ("sql" in content.lower())
                            or ("query" in content.lower())
                        )
                    ):
                        if "query" in args:
                            query = args.get("query")
                            generated_sql_queries.append(query)
                        elif "queries" in args:
                            queries = args.get("queries")
                            if isinstance(queries, list):
                                generated_sql_queries.extend(queries)
                            else:
                                query = str(queries)
                                generated_sql_queries.append(query)
                        elif (
                            "SELECT" in content.upper()
                            or "FROM" in content.upper()
                            or "WHERE" in content.upper()
                        ):
                            generated_sql_queries.append(content)

        except json.JSONDecodeError:
            continue

    return tool_messages, generated_sql_queries, selected_datasets


async def process_single_test_case(query, chain, server_url=None):
    query_copy = query.copy()
    expected_result = query_copy.get("expected_result", {})
    if "expected_result" in query_copy:
        del query_copy["expected_result"]

    test_url = server_url if server_url else url

    print(f"Processing query: {query['messages'][0]['content']}")
    results = {
        "query": query["messages"][0]["content"],
        "passed": False,
        "reasoning": "",
        "used_datasets": [],
        "sql_query_count": 0,
        "expected_dataset": "",
        "expected_sql_count": expected_result.get(
            "sql_query_count", "Not specified"
        ),
    }

    try:
        response = requests.post(
            test_url,
            json={
                **query_copy,
                "chat_id": "",
                "trace_id": "",
            },
            headers={
                "Content-Type": "application/json",
                "Accept": "text/event-stream",
            },
            stream=True,
        )
        response.raise_for_status()

        final_response = ""
        tool_messages = []
        generated_sql_queries = []
        selected_datasets = []

        for line in response.iter_lines():
            if not line:
                continue

            decoded_line = line.decode("utf-8").strip()
            if not decoded_line.startswith("data:"):
                continue

            if decoded_line == "data: [DONE]":
                break

            try:
                chunk_data = json.loads(decoded_line[len("data: ") :])

                if "choices" in chunk_data and chunk_data["choices"]:
                    delta = chunk_data["choices"][0].get("delta", {})

                    if "content" in delta and delta["content"] is not None:
                        final_response += delta["content"] or ""

                    tool_calls = delta.get("tool_calls")
                    if tool_calls is not None:
                        msgs, queries, datasets = process_tool_calls(
                            tool_calls
                        )
                        tool_messages.extend(msgs)
                        generated_sql_queries.extend(queries)
                        selected_datasets.extend(datasets)
            except json.JSONDecodeError:
                continue

        final_response += f"\n     used_datasets: {selected_datasets}"
        final_response += (
            f"\n     generated_sql queries: {generated_sql_queries}"
        )
        final_response += f"\n     tool_messages: {tool_messages}"

        print(f"Final AI Response: {final_response}")

        result = await chain.ainvoke(
            {
                "generated_answer": final_response,
                "expected_result": expected_result,
            }
        )

        results["reasoning"] = result.get("reasoning", "No reasoning provided")
        results["used_datasets"] = selected_datasets
        results["sql_query_count"] = len(generated_sql_queries)

        if "dataset_identified" in expected_result:
            results["expected_dataset"] = expected_result["dataset_identified"]
        else:
            results[
                "expected_dataset"
            ] = "Not applicable for single dataset case"

        if result["correct"] == "true":
            results["passed"] = True
            print("✅ Query passed")
        elif result["correct"] == "partial":
            results["passed"] = "partial"
            print("🟡 Query partially correct")
            print(f"Reasoning: {results['reasoning']}")
        else:
            print("❌ Query failed")
            print(f"Reasoning: {results['reasoning']}")

        return results

    except Exception as e:
        print(f"API request failed: {str(e)}")
        print(traceback.format_exc())
        results["reasoning"] = f"Error: {str(e)}"
        return results


def get_test_cases(test_type="all"):
    if test_type == "single":
        return SINGLE_DATASET_TEST_CASES
    elif test_type == "multi":
        return COMPLEX_QUERY_CASES
    else:
        return SINGLE_DATASET_TEST_CASES + COMPLEX_QUERY_CASES


async def run_tests(
    test_type="all", server_url="http://localhost:8001/api/v1/chat/completions"
):
    print(f"\nRunning {test_type} dataset tests against {server_url}")

    chain = create_chain()
    test_cases = get_test_cases(test_type)
    results = []

    print(f"Running {len(test_cases)} test cases")

    for query in test_cases:
        print(f"Running test case: {query['messages'][0]['content']}")
        result = await process_single_test_case(query, chain, server_url)
        results.append(result)

    passed = sum(1 for r in results if r["passed"] is True)
    partial = sum(1 for r in results if r["passed"] == "partial")
    failed = sum(1 for r in results if r["passed"] is False)

    print("\n== Results Summary ==")
    print(f"Total tests: {len(results)}")
    print(f"Passed: {passed}")
    print(f"Partial: {partial}")
    print(f"Failed: {failed}")

    if failed > 0 or partial > 0:
        print("\n== Failed Tests ==")
        for ft in results:
            if ft["passed"] is not True:
                print(f"Query: {ft['query']}")
                print(f"Reason: {ft['reasoning']}\n")

    return results


if __name__ == "__main__":
    asyncio.run(run_tests(test_type="multi"))

# Example usage:
# To run all tests:
# asyncio.run(run_tests(test_type="all"))

# To run only single dataset tests:
# asyncio.run(run_tests(test_type="single"))
#
# To run only multi-dataset tests:
# asyncio.run(run_tests(test_type="multi"))
#
# To run against a custom server:
# asyncio.run(run_tests(server_url="http://custom-server:8001/api/v1/chat/completions"))
