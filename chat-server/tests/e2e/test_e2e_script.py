import asyncio
import json
import os
import traceback
from datetime import datetime

import requests
from dotenv import load_dotenv
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from portkey_ai import PORTKEY_GATEWAY_URL, createHeaders

from app.core.constants import (
    DATASETS_USED,
    INTERMEDIATE_MESSAGES,
    SQL_QUERIES_GENERATED,
)

from .multi_dataset_cases import MULTI_DATASET_TEST_CASES
from .single_dataset_cases import SINGLE_DATASET_TEST_CASES
from .terminal_formatter import TerminalFormatter

load_dotenv()

url = "http://localhost:8001/api/v1/chat/completions"


def setup_model():
    """
    Initialize and return a ChatOpenAI model configured with environment-based API credentials and provider.
    
    The model uses the "gpt-4o-mini" variant and is set up to communicate with the gateway URL specified by `PORTKEY_GATEWAY_URL`.
    """
    api_key = os.getenv("PORTKEY_API_KEY")
    provider = os.getenv("PORTKEY_PROVIDER_NAME")

    model = ChatOpenAI(
        api_key="X",  # type: ignore
        base_url=PORTKEY_GATEWAY_URL,
        default_headers=createHeaders(
            api_key=api_key,
            provider=provider,
        ),
        model="gpt-4o-mini",
    )
    return model


def create_chain():
    template = """
        You are a judgemental data analyst assistant.

        You will be given:
        1. The answer generated by our system in response to a user query
        2. Details about the expected result

        generated_answer: {generated_answer}
        expected_result: {expected_result}

        The expected_result may contain some or all of these details:
        - dataset_identified: which dataset should be used (only present for multi-dataset cases)
        - sql_query_count: number of SQL queries expected
        - visualization_needed: whether visualization is required
        - visualization_type: what type of visualization is appropriate

        Your task is to evaluate if the generated answer meets the expected criteria:

        RETURN 'true' if:
        - The answer correctly identified the expected dataset (if dataset_identified is specified)
        - The number of SQL queries used is appropriate (matching sql_query_count if specified)
        - Visualization recommendations (if applicable) match the expected result
        - The numerical values are within acceptable ranges
        - The SQL queries were properly extracted and reported in the response

        RETURN 'false' if:
        - The answer identified the wrong dataset (when dataset_identified is specified)
        - The SQL query count is significantly off
        - Visualization was needed but not recommended (or vice versa)
        - The answer is factually incorrect
        - No SQL queries were detected when they should have been

        RETURN 'partial' if:
        - The answer is factually correct but there are minor issues with:
          - Dataset identification (when applicable)
          - SQL query count
          - Visualization recommendations
        - Or if the answer is partially correct but missing some components

        RETURN the response as JSON object with the following keys:
        {{
            "correct": "true" | "false" | "partial",
            "reasoning": "reasoning for why the answer is incorrect or partial,
                          no reasoning needed for correct answers",
        }}
    """

    prompt = ChatPromptTemplate.from_template(template)
    parser = JsonOutputParser()
    llm = setup_model()
    return prompt | llm | parser


def process_tool_calls(tool_calls):
    """
    Extracts tool messages, generated SQL queries, and selected datasets from a list of tool call dictionaries.
    
    Parameters:
        tool_calls (list): List of dictionaries representing tool calls from AI responses.
    
    Returns:
        tuple: A tuple containing three lists:
            - tool_messages (list): Intermediate messages with role "intermediate".
            - generated_sql_queries (list): SQL queries extracted from tool calls.
            - selected_datasets (list): Names or identifiers of datasets referenced in tool calls.
    """
    tool_messages = []
    generated_sql_queries = []
    selected_datasets = []

    if not tool_calls:
        return tool_messages, generated_sql_queries, selected_datasets

    for tool_call in tool_calls:
        if not tool_call or "function" not in tool_call:
            continue

        function_data = tool_call.get("function", {})
        name = function_data.get("name")

        try:
            args = json.loads(function_data.get("arguments", "{}"))

            if name == SQL_QUERIES_GENERATED:
                if "query" in args:
                    query = args.get("query")
                    generated_sql_queries.append(query)
                elif "queries" in args:
                    queries = args.get("queries")
                    if isinstance(queries, list):
                        generated_sql_queries.extend(queries)
                    else:
                        query = str(queries)
                        generated_sql_queries.append(query)

            elif name == DATASETS_USED:
                datasets = args.get("datasets", [])
                if datasets:
                    if isinstance(datasets, list):
                        selected_datasets.extend(datasets)
                    else:
                        selected_datasets.append(str(datasets))

            elif name == INTERMEDIATE_MESSAGES:
                content = args.get("content", "")
                category = args.get("category", "")
                if args.get("role") == "intermediate":
                    tool_messages.append(content)

                    if category == DATASETS_USED or (content and "dataset" in content.lower()):
                        if "datasets" in args:
                            datasets = args.get("datasets")
                            if isinstance(datasets, list):
                                selected_datasets.extend(datasets)
                            else:
                                selected_datasets.append(str(datasets))
                        elif ":" in content:
                            if (
                                "using dataset" in content.lower()
                                or "selected dataset" in content.lower()
                            ):
                                datasets_part = content.split(":", 1)[1].strip()
                                selected_datasets.append(datasets_part)

                    if category == SQL_QUERIES_GENERATED or (
                        content and (("sql" in content.lower()) or ("query" in content.lower()))
                    ):
                        if "query" in args:
                            query = args.get("query")
                            generated_sql_queries.append(query)
                        elif "queries" in args:
                            queries = args.get("queries")
                            if isinstance(queries, list):
                                generated_sql_queries.extend(queries)
                            else:
                                query = str(queries)
                                generated_sql_queries.append(query)
                        elif (
                            "SELECT" in content.upper()
                            or "FROM" in content.upper()
                            or "WHERE" in content.upper()
                        ):
                            generated_sql_queries.append(content)

        except json.JSONDecodeError:
            continue

    return tool_messages, generated_sql_queries, selected_datasets


async def process_single_test_case(
    query, chain, formatter, server_url=None, test_number=None, total_tests=None
):
    """
    Asynchronously processes a single test case by sending a query to the conversational AI server, streaming and aggregating the response, extracting relevant tool outputs, and evaluating the result using the provided evaluation chain and formatter.
    
    Parameters:
        query (dict): The test case containing the user query and expected result.
        chain: The evaluation chain used to assess the AI's response.
        formatter: Formatter instance for structured terminal output.
        server_url (str, optional): The server endpoint for chat completions. Defaults to a preset URL if not provided.
        test_number (int, optional): The index of the current test case.
        total_tests (int, optional): The total number of test cases in the suite.
    
    Returns:
        dict: A dictionary containing the test query, pass status, evaluation reasoning, used datasets, SQL query count, expected dataset, and expected SQL query count.
    """
    query_copy = query.copy()
    expected_result = query_copy.get("expected_result", {})
    if "expected_result" in query_copy:
        del query_copy["expected_result"]

    test_url = server_url if server_url else url

    test_query = query["messages"][0]["content"]
    formatter.print_test_case_header(test_number, total_tests, test_query)

    results = {
        "query": test_query,
        "passed": False,
        "reasoning": "",
        "used_datasets": [],
        "sql_query_count": 0,
        "expected_dataset": "",
        "expected_sql_count": expected_result.get("sql_query_count", "Not specified"),
    }

    try:
        formatter.print_processing_status("Processing query...")

        response = requests.post(
            test_url,
            json={
                **query_copy,
                "chat_id": "",
                "trace_id": "",
            },
            headers={
                "Content-Type": "application/json",
                "Accept": "text/event-stream",
            },
            stream=True,
        )
        response.raise_for_status()

        final_response = ""
        tool_messages = []
        generated_sql_queries = []
        selected_datasets = []

        for line in response.iter_lines():
            if not line:
                continue

            decoded_line = line.decode("utf-8").strip()
            if not decoded_line.startswith("data:"):
                continue

            if decoded_line == "data: [DONE]":
                break

            try:
                chunk_data = json.loads(decoded_line[len("data: ") :])

                if "choices" in chunk_data and chunk_data["choices"]:
                    delta = chunk_data["choices"][0].get("delta", {})

                    if "content" in delta and delta["content"] is not None:
                        final_response += delta["content"] or ""

                    tool_calls = delta.get("tool_calls")
                    if tool_calls is not None:
                        msgs, queries, datasets = process_tool_calls(tool_calls)
                        tool_messages.extend(msgs)
                        generated_sql_queries.extend(queries)
                        selected_datasets.extend(datasets)
            except json.JSONDecodeError:
                continue

        formatter.print_response_summary(
            final_response, selected_datasets, generated_sql_queries, tool_messages
        )

        formatter.print_evaluation_status()

        result = await chain.ainvoke(
            {
                "generated_answer": final_response,
                "expected_result": expected_result,
            }
        )

        results["reasoning"] = result.get("reasoning", "No reasoning provided")
        results["used_datasets"] = selected_datasets
        results["sql_query_count"] = len(generated_sql_queries)

        if "dataset_identified" in expected_result:
            results["expected_dataset"] = expected_result["dataset_identified"]
        else:
            results["expected_dataset"] = "Not applicable for single dataset case"

        if result["correct"] == "true":
            results["passed"] = True
            formatter.print_test_result("passed")
        elif result["correct"] == "partial":
            results["passed"] = "partial"
            formatter.print_test_result("partial", results["reasoning"])
        else:
            formatter.print_test_result("failed", results["reasoning"])

        return results

    except Exception as e:
        formatter.print_error(str(e), traceback.format_exc())
        results["reasoning"] = f"Error: {str(e)}"
        return results


def get_test_cases(test_type="all"):
    """
    Return test cases based on the specified dataset type.
    
    Parameters:
        test_type (str): Type of test cases to return. Accepts "single" for single-dataset cases, "multi" for multi-dataset cases, or any other value for all cases.
    
    Returns:
        list: A list of test case dictionaries matching the requested type.
    """
    if test_type == "single":
        return SINGLE_DATASET_TEST_CASES
    elif test_type == "multi":
        return MULTI_DATASET_TEST_CASES
    else:
        return SINGLE_DATASET_TEST_CASES + MULTI_DATASET_TEST_CASES


async def run_tests(test_type="all", server_url="http://localhost:8001/api/v1/chat/completions"):
    """
    Run all test cases of the specified type against the conversational AI server and print formatted results.
    
    Parameters:
        test_type (str): The category of test cases to run ("single", "multi", or "all").
        server_url (str): The URL of the chat completion API endpoint.
    
    Returns:
        results (list): A list of dictionaries containing the outcome of each test case.
    """
    start_time = datetime.now()

    formatter = TerminalFormatter(use_colors=True)

    formatter.print_framework_header(start_time)

    chain = create_chain()
    test_cases = get_test_cases(test_type)
    results = []

    formatter.print_test_suite_info(len(test_cases), test_type, server_url)

    for i, query in enumerate(test_cases, 1):
        result = await process_single_test_case(
            query, chain, formatter, server_url, i, len(test_cases)
        )
        results.append(result)

    formatter.print_results_summary(results, test_type, server_url, start_time)

    return results


if __name__ == "__main__":
    asyncio.run(run_tests(test_type="multi"))

# Example usage:
# To run all tests:
# asyncio.run(run_tests(test_type="all"))

# To run only single dataset tests:
# asyncio.run(run_tests(test_type="single"))
#
# To run only multi-dataset tests:
# asyncio.run(run_tests(test_type="multi"))
#
# To run against a custom server:
# asyncio.run(run_tests(server_url="http://custom-server:8001/api/v1/chat/completions"))
