import json
import os
from typing import Any, Dict, List, Optional, Tuple

import requests
from dotenv import load_dotenv
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from portkey_ai import PORTKEY_GATEWAY_URL, createHeaders

from app.core.constants import (
    DATASETS_USED,
    DATASETS_USED_ARG,
    INTERMEDIATE_MESSAGES,
    SQL_QUERIES_GENERATED,
    SQL_QUERIES_GENERATED_ARG,
    VISUALIZATION_RESULT,
    VISUALIZATION_RESULT_ARG,
)

from .terminal_formatter import TerminalFormatter

load_dotenv()

REQUEST_TIMEOUT = 120


def setup_model() -> ChatOpenAI:
    api_key = os.getenv("PORTKEY_API_KEY")
    provider = os.getenv("PORTKEY_PROVIDER_NAME")

    return ChatOpenAI(
        api_key="X",  # type: ignore
        base_url=PORTKEY_GATEWAY_URL,
        default_headers=createHeaders(
            api_key=api_key,
            provider=provider,
            metadata={
                "_user": "e2e_test",
            },
            trace_id="e2e_test",
            chat_id="e2e_test",
        ),
        model="gpt-4o",
    )


def create_evaluation_chain():
    template = """You are a fair and intelligent data analyst assistant evaluating query responses.

You will be given:
1. A comprehensive response object containing:
   - ai_response: The text response generated by our system
   - datasets_used: List of datasets identified and used
   - sql_queries_generated: List of SQL queries generated and executed
   - processing_steps: List of processing steps taken
   - visualization_results: Any visualization outputs (if applicable)
   - metadata: Summary counts of the above components
2. Details about the expected result (can be a string description or structured data)

generated_answer: {generated_answer}
expected_result: {expected_result}

The generated_answer is now a comprehensive object that includes:
- ai_response: The actual text response to evaluate
- datasets_used: Array of dataset IDs/names used in the analysis
- sql_queries_generated: Array of SQL queries executed
- processing_steps: Array of processing steps and intermediate messages
- visualization_results: Array of visualization outputs
- metadata: Object with counts (dataset_count, sql_query_count, etc.)

The expected_result may be:
- A string describing what the query should return (for comprehensive test cases)
- A structured object with specific criteria like dataset_identified, sql_query_count, visualization_needed, etc.

IMPORTANT EVALUATION PRINCIPLES:
1. DO NOT expect exact numerical values to match - the test cases use schema-based data, not real values
2. Focus on query approach, data structure, and result format rather than specific amounts or counts
3. If the expected result shows examples (like "including: A, B, C") and the generated answer provides a MORE COMPREHENSIVE list that includes those examples plus additional valid items, this should be marked as 'true'
4. Focus on whether the generated answer correctly addresses the query intent and uses the right data analysis approach
5. For list queries, a more complete answer is better than a partial one, as long as the items are relevant and correct
6. Evaluate based on whether the response demonstrates correct understanding of the data relationships and query requirements
7. **IMPORTANT**: Now evaluate the COMPLETE response including datasets used, SQL queries, and processing approach, not just the text response
8. Check if the datasets_used array contains the expected datasets mentioned in the expected_result
9. Verify if the sql_query_count matches expectations (if specified in expected_result)
10. Consider the processing_steps to understand if the right analytical approach was taken

RETURN 'true' if:
- For string expected results: The ai_response addresses the key points AND the datasets/queries show correct approach
- The answer provides equal or MORE comprehensive information than expected (especially for list queries)
- For structured expected results: The answer meets all specified criteria (dataset identification, SQL query count, visualization needs)
- The datasets_used array contains the expected datasets (if specified)
- The sql_query_count matches or appropriately exceeds expectations (if specified)
- The processing_steps demonstrate correct analytical workflow
- The answer follows the expected format/structure even if specific values differ
- The answer demonstrates correct understanding of the data relationships and query requirements
- The answer includes the expected types of information AND provides additional relevant details
- The response uses appropriate data analysis approach for the query type

RETURN 'false' if:
- The answer is completely irrelevant or doesn't address the query at all
- For structured results: Wrong dataset identified, significantly incorrect SQL query count, missing required visualizations
- The datasets_used array is missing expected datasets or contains completely wrong datasets
- The sql_query_count is significantly wrong (e.g., expected 2 queries but only 1 was generated)
- The answer shows fundamental misunderstanding of the query intent or data relationships
- The response uses completely wrong data analysis approach
- The answer provides a fundamentally incorrect interpretation of the data

RETURN 'partial' if:
- The answer uses correct approach but has minor format/presentation issues
- Some but not all requirements are met but the core query is addressed correctly
- The datasets_used is partially correct (some right datasets but missing others)
- The sql_query_count is close to expected but not exact
- The answer demonstrates understanding but has implementation gaps
- The response provides relevant information but misses some key aspects
- The answer provides some but not all expected types of information in a complex query

Return the response as JSON object:
{{
    "correct": "true" | "false" | "partial",
    "reasoning": "reasoning for why the answer is incorrect or partial, no reasoning needed for correct answers"
}}"""

    prompt = ChatPromptTemplate.from_template(template)
    return prompt | setup_model() | JsonOutputParser()


def process_tool_calls(
    tool_calls: List[Dict[str, Any]],
) -> Tuple[List[str], List[str], List[str], List[str]]:
    tool_messages = []
    generated_sql_queries = []
    selected_datasets = []
    visualization_results = []

    if not tool_calls:
        return tool_messages, generated_sql_queries, selected_datasets, visualization_results

    for tool_call in tool_calls:
        if not tool_call or "function" not in tool_call:
            continue

        function_data = tool_call.get("function", {})
        name = function_data.get("name")

        try:
            args = json.loads(function_data.get("arguments", "{}"))

            if name == SQL_QUERIES_GENERATED:
                _extract_sql_queries(args, generated_sql_queries)

            elif name == DATASETS_USED:
                _extract_datasets(args, selected_datasets)

            elif name == VISUALIZATION_RESULT:
                _extract_visualization_results(args, visualization_results)

            elif name == INTERMEDIATE_MESSAGES and args.get("role") == "intermediate":
                content = args.get("content", "")
                category = args.get("category", "")
                tool_messages.append(content)

                if category == DATASETS_USED or (content and "dataset" in content.lower()):
                    _extract_datasets_from_content(args, content, selected_datasets)

                if category == SQL_QUERIES_GENERATED or _contains_sql_keywords(content):
                    _extract_sql_from_content(args, content, generated_sql_queries)

        except json.JSONDecodeError:
            continue

    return tool_messages, generated_sql_queries, selected_datasets, visualization_results


def _extract_sql_queries(args: Dict[str, Any], sql_queries: List[str]) -> None:
    if SQL_QUERIES_GENERATED_ARG in args and args.get(SQL_QUERIES_GENERATED_ARG):
        queries = args.get(SQL_QUERIES_GENERATED_ARG)
        if isinstance(queries, list):
            sql_queries.extend([str(q) for q in queries if q])
        elif queries:
            sql_queries.append(str(queries))


def _extract_datasets(args: Dict[str, Any], datasets: List[str]) -> None:
    if DATASETS_USED_ARG in args and args.get(DATASETS_USED_ARG):
        dataset_args = args.get(DATASETS_USED_ARG)
        if isinstance(dataset_args, list):
            datasets.extend(dataset_args)
        else:
            datasets.append(str(dataset_args))


def _extract_visualization_results(args: Dict[str, Any], visualization_results: List[str]) -> None:
    if VISUALIZATION_RESULT_ARG in args and args.get(VISUALIZATION_RESULT_ARG):
        viz_args = args.get(VISUALIZATION_RESULT_ARG)
        if isinstance(viz_args, list):
            visualization_results.extend([str(v) for v in viz_args if v])
        elif viz_args:
            visualization_results.append(str(viz_args))


def _extract_datasets_from_content(args: Dict[str, Any], content: str, datasets: List[str]) -> None:
    if "datasets" in args:
        _extract_datasets(args, datasets)
    elif ":" in content and any(
        keyword in content.lower() for keyword in ["using dataset", "selected dataset"]
    ):
        datasets_part = content.split(":", 1)[1].strip()
        datasets.append(datasets_part)


def _extract_sql_from_content(args: Dict[str, Any], content: str, sql_queries: List[str]) -> None:
    if "query" in args and args.get("query"):
        sql_queries.append(str(args.get("query")))
    elif "queries" in args:
        _extract_sql_queries(args, sql_queries)
    elif _contains_sql_keywords(content):
        sql_queries.append(content)


def _contains_sql_keywords(content: str) -> bool:
    return any(keyword in content.upper() for keyword in ["SELECT", "FROM", "WHERE"])


def get_user_query(test_case: Dict[str, Any]) -> str:
    if "messages" in test_case:
        for message in test_case["messages"]:
            if message.get("role") == "user":
                return message.get("content", "")
    return test_case.get("query", "")


async def send_chat_request(test_case: Dict[str, Any], url: str) -> Dict[str, Any]:
    query_copy = test_case.copy()
    query_copy.pop("expected_result", None)

    try:
        response = requests.post(
            url,
            json={**query_copy, "chat_id": "", "trace_id": ""},
            headers={"Content-Type": "application/json", "Accept": "text/event-stream"},
            stream=True,
            timeout=REQUEST_TIMEOUT,
        )
        response.raise_for_status()

        final_response = ""
        tool_messages = []
        generated_sql_queries = []
        selected_datasets = []
        visualization_results = []

        for line in response.iter_lines():
            if not line:
                continue

            decoded_line = line.decode("utf-8").strip()
            if not decoded_line.startswith("data:"):
                continue

            if decoded_line == "data: [DONE]":
                break

            try:
                chunk_data = json.loads(decoded_line[len("data: ") :])
                if "choices" in chunk_data and chunk_data["choices"]:
                    delta = chunk_data["choices"][0].get("delta", {})

                    if "content" in delta and delta["content"] is not None:
                        final_response += delta["content"] or ""

                    tool_calls = delta.get("tool_calls")
                    if tool_calls is not None:
                        msgs, queries, datasets, viz_results = process_tool_calls(tool_calls)
                        tool_messages.extend(msgs)
                        generated_sql_queries.extend(queries)
                        selected_datasets.extend(datasets)
                        visualization_results.extend(viz_results)
            except json.JSONDecodeError:
                continue

        return {
            "final_response": final_response,
            "tool_messages": tool_messages,
            "generated_sql_queries": generated_sql_queries,
            "selected_datasets": selected_datasets,
            "visualization_results": visualization_results,
        }

    except requests.exceptions.RequestException as e:
        return {"error": f"API request failed: {str(e)}"}


def initialize_test_results(user_query: str, expected_result: Any) -> Dict[str, Any]:
    """Initialize test results structure with proper handling of string expected results."""
    expected_sql_count = "Not specified"

    if isinstance(expected_result, dict):
        expected_sql_count = expected_result.get("sql_query_count", "Not specified")

    return {
        "query": user_query,
        "passed": False,
        "reasoning": "",
        "used_datasets": [],
        "sql_query_count": 0,
        "expected_dataset": "",
        "expected_sql_count": expected_sql_count,
        "status": "error",
    }


def handle_expected_error(
    results: Dict[str, Any], formatter: Optional[TerminalFormatter]
) -> Dict[str, Any]:
    results.update(
        {
            "evaluation": {"note": "Expected error test case - passed"},
            "status": "success",
            "passed": True,
        }
    )
    if formatter:
        formatter.print_success("Expected error test completed successfully")
    return results


def update_results_with_evaluation(
    results: Dict[str, Any],
    evaluation: Dict[str, Any],
    response: Dict[str, Any],
    expected_result: Any,  # Can be string or dict
    formatter: Optional[TerminalFormatter],
) -> None:
    results.update(
        {
            "reasoning": evaluation.get("reasoning", "No reasoning provided"),
            "used_datasets": response["selected_datasets"],
            "sql_query_count": len(response["generated_sql_queries"]),
            "response": response,
            "evaluation": evaluation,
        }
    )

    # Handle dataset identification for dict-based expected results
    if isinstance(expected_result, dict) and "dataset_identified" in expected_result:
        results["expected_dataset"] = expected_result["dataset_identified"]
    else:
        results["expected_dataset"] = "Not applicable for single dataset case"

    correct = evaluation["correct"]
    if correct == "true":
        results["passed"] = True
        results["status"] = "success"
        if formatter:
            formatter.print_test_result("passed")
    elif correct == "partial":
        results["passed"] = "partial"
        results["status"] = "success"
        if formatter:
            formatter.print_test_result("partial", results["reasoning"])
    else:
        results["status"] = "error"
        if formatter:
            formatter.print_test_result("failed", results["reasoning"])
